---
title: "Lab 4"
author: "Nathaniel Gunter"
date: "2/3/2020"
output: 
  html_document:
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(s20x)
```
## Task 1

```{r}
getwd()
```

## Task 2

```{r}
spruce.df<- read.csv("SPRUCE.csv")
tail(spruce.df)
```

## Task 3
```{r}
with(spruce.df, trendscatter(x=BHDiameter, y=Height, f=0.5))
```


```{r}
spruce.lm<-with(spruce.df,lm(Height~BHDiameter))
height.res<-with(spruce.df,residuals(spruce.lm))
height.fit<-fitted(spruce.lm)
plot(x=height.fit,y=height.res)
```

```{r}
trendscatter(x=height.fit,y=height.res,f=0.5)
```

The plot is a downward-facing quadratic, as opposed to the roughly square root like plot seen in Task 3.

```{r}
plot(spruce.lm, which=1)
```

```{r}
normcheck(spruce.lm,shapiro.wilk=TRUE)
```

The p-value for the Shapiro-Wilk is 0.29. The NULL hypothesis is that the population is a normal distribution. In this case, we expect that the residuals should be normal distribution so we should see a large p-value. Because the p-value is relatively low at 0.29, we conclude that a straight line is not a valid model for this data set.

## Task 4

```{r}
quad.lm = lm(Height~BHDiameter + I(BHDiameter^2),data=spruce.df)
```

```{r}
with(spruce.df,plot(Height~BHDiameter,bg="Blue",pch=21,cex=1.2,
ylim=c(0,max(Height)),xlim=c(0,max(BHDiameter))))

f<-function(x){quad.lm$coefficients[1]+quad.lm$coefficients[2]*x+quad.lm$coefficients[3]*x^2}

curve(f, lwd=2, col="steelblue",add=TRUE)
```

```{r}
quad.fit<-with(spruce.df,fitted(quad.lm))
plot(quad.lm,which=1)
```

```{r}
normcheck(quad.lm,shapiro.wil=TRUE)
```

The Shapiro-Wilk p-value is 0.684, which is higher than that for the linear model, so the quadratic model better describes the data. 

## Task 5

```{r}
summary(quad.lm)
```

The value of $\hat{\beta_0}$ is 0.860896, the value of $\hat{\beta_1}$ is 1.469592, and the value of $\hat{\beta_2}$ is -0.027457. Therefore, the equation of the fitted line is $y = 0.860896 + 1.469592 \times x - 0.027457 \times x^2$

```{r}
ciReg(quad.lm)
```

```{r}
predict(quad.lm,data.frame(BHDiameter=c(15,18,20)))
```

With the linear model, we predicted that the height at 15 cm in diameter was 16.36895, height at 18 cm in diameters was 17.81338, and height at 20 cm in diameter was 18.77632. The multiple $R^2$ for the quadratic model is 0.7741, larger than the linear multiple $R^2$ of 0.6569. Based on the adjusted $R^2$, the quadratic model is "better" for this data. The multiple $R^2$ is interpreted as the percentage of variance in the dependent variable that can be explained by the model. Based on this definition, the quadratic model explains the most variability in Height.

```{r}
anova(spruce.lm)
anova(quad.lm)
```

The quadratic model has an additional predictor that is given a very high sigificance based on ANOVA, so there must be something that the linear model is missing.

```{r}
height.qfit<-fitted(quad.lm)
RSS <- with(spruce.df,sum(Height-height.qfit)^2)
MSS<-with(spruce.df,sum((height.qfit-mean(Height))^2))
TSS<-with(spruce.df,sum((Height-mean(Height))^2))
RSS
MSS
TSS
MSS/TSS
```

## Task 6

```{r}
cooks20x(quad.lm)
```

The cooks distance for a given observation is a measure of how much the model changes when that observation is removed from the data set. This gives an estimate of the importance of potential outliers. The larger the cooks distance, the more influential, and therefore more interesting, the point. For our quadratic model, the cooks distance tells us that the 24th observation is the most influential, so we may want to investigate it as a potential outlier.


```{r}
quad2.lm=lm(Height~BHDiameter+I(BHDiameter^2),data=spruce.df[-24,])
summary(quad2.lm)
summary(quad.lm)
```

After removing the 24th observation, the adjusted $R^2$ increases by approximately 0.04, suggesting that the removal improved how well a quadratic model fit the data. Additionally, the p-values for individual predictors decreased, suggesting again that the data is better described. Therefore, we can conclude that the cooks distance can be used to identify potential outliers that make the data easer to fit when removed.

## Task 7

The graph is represented by two lines

$$
l_1 : y=\beta_0 + \beta_1 x, \  \ x\leq x_k \\
l_2 : y=\beta_0 + \delta + (\beta_1+\xi)x, \ x> x_k .
$$

At the intersection (called the knot), $y=y_k$ and $x = x_k$ for both of the lines. By substitution, we can then see that 

$$
y_1 = y_2 \\
\implies \beta_0 + \beta_1 x_k = \beta_0 + \delta + (\beta_1+\xi)x_k\\
\implies \delta = - \xi x_k
$$
Then we can write line two as 

$$
y_2=\beta_0-\xi x_k+(\beta_1+\xi)x \\
\ = \beta_0-\xi x_k + \beta_1 x + \xi x \\
\ =\beta_0+\beta_1 x + \xi (x-x_k) \\
\ = l_1 + \xi (x-x_k)
$$

Now we want the function to be $l_1$ for $x\leq x_k$ and $l_2$ for $x>x_k$. We can do this with an indicator function $I(x_k)$ such that 
$$ 
I(x_k)=\left\{
\begin{array}{ll}
      1 & x> x_k \\
      0 & x\leq x_k \\
\end{array} 
\right..
$$

If we multiply the $\xi (x-x_k)$ by the indicator function, we then recover $l_1$ and $l_2$ where we want to and obtain the final expression 
$$
y = \beta_0 + \beta_1 x + I(x_k)\xi (x-x_k).
$$

```{r}
sp2.df=within(spruce.df, X<-(BHDiameter-18)*(BHDiameter>18))
lmp=lm(Height~BHDiameter + X,data=sp2.df)
tmp=summary(lmp)
myf = function(x,coef){
  coef[1]+coef[2]*(x) + coef[3]*(x-18)*(x-18>0)
}
plot(spruce.df,main="Piecewise regression")
curve(myf(x,coef=tmp$coefficients[,"Estimate"] ),add=TRUE, lwd=2,col="Blue")
abline(v=18)
text(18,12,paste("R sq.=",round(tmp$r.squared,4) ))
```

## Task 8

```{r}
library(math4753)
dird = "C:/Users/Nathaniel Gunter/Documents/School/Junior Year/Spring/Applied Statistical Methods/Data/DATAxls/"
csv = "ACCIDENTS.csv"
acc = myread(csv,dird)
head(acc)
```